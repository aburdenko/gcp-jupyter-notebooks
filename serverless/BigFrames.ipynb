{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install bigframes --quiet\n","!pip install SQLAlchemy --quiet\n","!pip install sqlalchemy-bigquery --quiet\n","\n","import IPython\n","print( 'restarting kernel...' )\n","app = IPython.Application.instance()\n","app.kernel.do_shutdown(True)"],"metadata":{"id":"XQm8oP5Gdecr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Specify Project details and LOCATION of the BQ table\n","\n","PROJECT_ID = \"\"  # @param {type:\"string\"}\n","DB_PROJECT_ID = \"bigquery-public-data\"  # @param {type:\"string\"}\n","\n","#DB_PROJECT_ID = \"cloud-llm-preview1\"  # @param {type:\"string\"}\n","LOCATION = \"us-central1\"  # @param {type:\"string\"}\n","#DATASET_ID = 'blackbelt_capstone_healthcare' # @param {type:\"string\"}\n","DATASET_ID = 'ml_datasets' # @param {type:\"string\"}\n","\n","#@markdown ### Enter the topic name and subscription to be used for pub/sub\n","TOPIC_NAME=\"customer-chat\" # @param {type:\"string\"}\n","SUBSCRIPTION=\"colab-sub\" # @param {type:\"string\"}\n","OUTPUT_TABLE_NAME=\"penguins_model\" # @param {type:\"string\"}\n","\n","\n","import sys\n","IN_COLAB = 'google.colab' in sys.modules\n","%env IN_COLAB=$IN_COLAB\n","\n","!gcloud config set project $PROJECT_ID -q\n","!gcloud config get project"],"metadata":{"id":"3RwuXImotMXF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sqlalchemy import *\n","from sqlalchemy.engine import create_engine\n","from sqlalchemy.schema import *\n","from google.cloud import bigquery\n","\n","def create_ds_view( tbl_name ):\n","  view_name = f\"vw_{tbl_name}\"\n","  view_id = f\"{PROJECT_ID}.{DATASET_ID}.{view_name}\"\n","  source_id = f\"{DB_PROJECT_ID}.{DATASET_ID}.{tbl_name}\"\n","  view = bigquery.Table(view_id)\n","\n","  view.view_query = f\"SELECT * FROM `{source_id}`\"\n","\n","  # Make an API request to create the view.\n","  view = bq_client.create_table(view, exists_ok=True)\n","  #print(f\"Created {view.table_type}: {str(view.reference)}\")\n","  fq_view_name = f\"{DATASET_ID}.{view_name}\"\n","  return view_name, fq_view_name\n","\n","\n","\n","query = f\"\"\"SELECT table_name \\\n","  FROM `{DB_PROJECT_ID}.{DATASET_ID}`.INFORMATION_SCHEMA.COLUMNS \"\"\"\n","\n","\n","# Create a BigQuery client.\n","bq_client = bigquery.Client(project=PROJECT_ID)\n","\n","table_uri = f\"bigquery://{PROJECT_ID}/{DATASET_ID}\"\n","engine = create_engine(\n","    f\"bigquery://{DB_PROJECT_ID}/{DATASET_ID}?user_supplied_client=True\",\n","    connect_args={'client': bq_client}\n",")\n","\n","tbl_names = set(engine.execute(query).unique().fetchall())\n","from itertools import chain\n","tbl_names = list(chain(*tbl_names))\n","\n","# Create the dataset if it doesn't exist\n","try:\n","    bq_client.get_dataset(DATASET_ID)\n","except:\n","    # The dataset doesn't exist, so create it.\n","    dataset = bigquery.Dataset(f\"{PROJECT_ID}.{DATASET_ID}\")\n","    bq_client.create_dataset(dataset)\n","\n","\n","view_struct = list(map( create_ds_view, tbl_names ))\n","\n","view_names = [view[0] for view in view_struct]\n","print(view_names)\n","fq_view_names = [view[1] for view in view_struct]\n","\n","\n","table_str = \"','\".join(tbl_names)\n","view_str = \"','\".join(view_names)\n","\n","column_query =  f\"SELECT table_name, column_name \\\n","  FROM `{PROJECT_ID}.{DATASET_ID}`.INFORMATION_SCHEMA.COLUMNS \\\n","  WHERE table_name in ('{view_str}')\"\n","\n","print(column_query)\n","\n","columns =  list(engine.execute(column_query).unique().fetchall())\n","#print(columns[1])\n","columns = list(chain(*columns))\n","#print(columns)\n","\n","column_names = list(map(lambda x, y: f\"{x}.{y}\", columns[::2], columns[1::2]))"],"metadata":{"id":"ZyVl-9lPtLSq"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XwVoPbhSdcYW"},"outputs":[],"source":["# Start a BigFrames session\n","import bigframes\n","\n","ops = bigframes.BigQueryOptions()\n","ops.project = PROJECT_ID\n","ops.dataset = \"ml_datasets\"\n","ops.table = \"vw_penguins\"\n","\n","# Connect to a BigQuery session\n","session = bigframes.connect(context=ops)\n","\n","# Initialize a dataframe for a BigQuery table\n","df = session.read_gbq(\"bigquery-public-data.ml_datasets.penguins\")\n","\n","# View the DataFrame\n","df\n","\n","# View the column names in the dataframe (aka columns names in the table)\n","df.columns\n","\n","# View the table schema\n","df.dtypes\n","# Select a subset of columns\n","df = df[[\n","    \"species\",\n","    \"island\",\n","    \"body_mass_g\",\n","]]\n","df\n","\n","# View the first ten values of a series\n","df['body_mass_g'].head(10)\n","\n","# Compute the mean of a series\n","df['body_mass_g'].mean()\n","\n","# Filter the dataframe\n","df[df['body_mass_g'] >= 4000.0]\n","\n","#####################################\n","#       Remote Functions            #\n","#####################################\n","\n","# BigFrames gives you the ability to turn your custom scalar functions into a BigQuery remote function. It requires the GCP project to be set up appropriately and the user having sufficient privileges to use them. One can find more details on it via help command.\n","#help(bigframes.remote_function)\n","\n","# Run the custom function on the BigFrames dataframe\n","@session.remote_function([float], str, bigquery_connection='bigframes-rf-conn')\n","def get_bucket(num):\n","    if not num: return \"NA\"\n","    boundary = 4000\n","    return \"at_or_above_4000\" if num >= boundary else \"below_4000\"\n","\n","#####################################\n","#       ML API                      #\n","#####################################\n","\n","# Start a session and initialize a dataframe for a BigQuery table\n","df = session.read_gbq(\"bigquery-public-data.ml_datasets.penguins\")\n","df\n","\n","# Clean and prepare the data\n","# filter down to the data we want to analyze\n","adelie_data = df[df.species == \"Adelie Penguin (Pygoscelis adeliae)\"]\n","\n","# drop the columns we don't care about\n","adelie_data = adelie_data.drop(columns=[\"species\"])\n","\n","# drop rows with nulls to get our training data\n","training_data = adelie_data.dropna()\n","\n","# take a peek at the training data\n","training_data\n","\n"]},{"cell_type":"code","source":["# pick feature columns and label column\n","#feature_columns = training_data[['island', 'culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'sex']]\n","feature_columns = training_data[['island', 'culmen_length_mm', 'flipper_length_mm', 'sex']]\n","feature_columns.head()\n","\n","label_columns = training_data[['body_mass_g']]\n","\n","# also get the rows that we want to make predictions for (i.e. where the feature column is null)\n","missing_body_mass = adelie_data[adelie_data.body_mass_g.isnull()]\n","\n","# Train and evaluate a linear regression model using the ML API\n","from bigframes.ml.linear_model import LinearRegression\n","\n","# as in scikit-learn, a newly created model is just a bundle of parameters\n","# default parameters are fine here\n","model = LinearRegression()\n","\n","# this will train a temporary model in BigQuery Machine Learning\n","model.fit(feature_columns, label_columns)\n","\n","# check how the model performed, using the automatic test/training data split chosen by BQML\n","model.score(feature_columns, label_columns)\n","\n","# Make predictions using the model\n","model.predict(missing_body_mass)\n","\n","# Create the dataset if it doesn't exist\n","try:\n","    bq_client.get_dataset(DATASET_ID)\n","except:\n","    # The dataset doesn't exist, so create it.\n","    dataset = bigquery.Dataset(f\"{PROJECT_ID}.{DATASET_ID}\")\n","    bq_client.create_dataset(dataset)\n","\n","\n","# Save the trained model to BigQuery, so we can load it later\n","model.to_gbq(f\"{DATASET_ID}.{OUTPUT_TABLE_NAME}\", replace=True)"],"metadata":{"id":"Hua1wV1_vqRu"},"execution_count":null,"outputs":[]}]}