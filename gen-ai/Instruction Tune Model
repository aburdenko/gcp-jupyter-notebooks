{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1rd4w9Z8QW6BciGPp2pK9Jf8Ib0udLf6a","timestamp":1710880109579},{"file_id":"14xmkn6KjYliNgOz-CyIjfpeP0d3vGc5k","timestamp":1709240411384},{"file_id":"13LZMrj3RFNZo5CDq_iMLTUI60jqtMElR","timestamp":1687793994588}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"359697d5"},"source":["# Instruction Tune Model\n","---\n","\n","**Document AI üìëüëÄ + LangChain ü¶úÔ∏èüîó + ScaNN / Matching Engine üß©üîç**\n","\n","---\n","\n","| | |\n","|----------|-------------|\n","| Author(s)   | Alex Burdenko |\n","| Last updated | 2/29/2024 |\n"]},{"cell_type":"markdown","source":["\n","## Objective\n","\n","This notebook demonstrates how you would instruction tune a Google LLM using labels extracted by Doc AI.\n","\n","## Costs\n","\n","This tutorial uses billable components of Google Cloud Platform (GCP):\n","\n","-   [Vertex AI LLM APIs](https://cloud.google.com/vertex-ai/pricing#generative_ai_models)\n","-   [Cloud Storage [optional]](https://cloud.google.com/storage)"],"metadata":{"id":"fK2g6SbHdUn1"}},{"cell_type":"markdown","source":["## Install dependencies"],"metadata":{"id":"40hvToVo7DR_"}},{"cell_type":"code","source":["import sys\n","import subprocess\n","import pkg_resources\n","\n","#'google-cloud-aiplatform==1.26.1'\n","\n","# , 'Pillow==9.5.0'\n","#   , 'PyCryptodome'\n","\n","required = {\n","   'google-cloud-aiplatform'\n","  , 'google-cloud-documentai==2.16.0'\n","  , 'scann==1.2.9'\n","  , 'langchain==0.0.214'\n","  , 'pypdf==3.11.0'\n","  , 'gradio==3.35.2'\n","  , 'matplotlib==3.7.1'\n","  , 'shapely==1.8.5'\n","  , 'google-cloud-documentai'\n","  , 'google-auth'\n","}\n","\n","print(f\"working set entries: {pkg_resources.working_set}\")\n","\n","specific_required = set()\n","\n","for pkg in required:\n","  if not '==' in pkg and pkg in pkg_resources.working_set.by_key.keys():\n","    str_key=str( pkg_resources.get_distribution(pkg) ).replace(' ','==')\n","    print(f\"adding {str_key}\")\n","    specific_required.add(str_key)\n","  else:\n","    specific_required.add(pkg)\n","\n","required=specific_required\n","\n","required.discard(None)\n","installed = { f\"{pkg.key}=={pkg_resources.get_distribution(pkg.key).version}\" for pkg in pkg_resources.working_set}\n","missing = required - installed\n","print(f\"required: {required}\")\n","print(f\"installed: {installed}\")\n","#missing = (required - installed) | missing\n","print(f\"missing: {missing}\")\n","if missing:\n","    python = sys.executable\n","    print(f\"installing...\")\n","    #subprocess.check_call([python, '-m', 'pip', 'install', '--upgrade', f\"--target={PYTHON_LIB_PATH}\", *missing], stdout=subprocess.DEVNULL)\n","    subprocess.check_call([python, '-m', 'pip', 'install', '--upgrade', *missing], stdout=subprocess.DEVNULL)\n","    print(f\"done.\")\n","\n","    import IPython\n","    import time\n","    print( 'restarting kernel...' )\n","    app = IPython.Application.instance()\n","    app.kernel.do_shutdown(False)\n","\n","    # Wait for the kernel to \"crash\". Haven't found a more elegant workaround for do_shutdown being an async call..."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6-0lAZPunrvI","executionInfo":{"status":"ok","timestamp":1710880331775,"user_tz":240,"elapsed":131282,"user":{"displayName":"Alex Burdenko","userId":"04128376985135405510"}},"outputId":"536181c1-5cd5-4469-9270-a61cfc020797"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["working set entries: <pkg_resources.WorkingSet object at 0x7cf8d5e08d60>\n","adding google-auth==2.27.0\n","adding google-cloud-aiplatform==1.44.0\n","required: {'langchain==0.0.214', 'shapely==1.8.5', 'gradio==3.35.2', 'google-auth==2.27.0', 'google-cloud-documentai==2.16.0', 'matplotlib==3.7.1', 'scann==1.2.9', 'pypdf==3.11.0', 'google-cloud-documentai', 'google-cloud-aiplatform==1.44.0'}\n","installed: {'fastprogress==1.0.3', 'dbus-python==1.2.18', 'pyproj==3.6.1', 'ipython-genutils==0.2.0', 'grpcio==1.62.1', 'folium==0.14.0', 'contextlib2==21.6.0', 'blis==0.7.11', 'imbalanced-learn==0.10.1', 'dopamine-rl==4.0.6', 'langcodes==3.3.0', 'types-pytz==2024.1.0.20240203', 'ipython==7.34.0', 'triton==2.2.0', 'tensorflow-hub==0.16.1', 'sphinxcontrib-qthelp==1.0.7', 'tables==3.8.0', 'cvxpy==1.3.3', 'murmurhash==1.0.10', 'srsly==2.4.8', 'catalogue==2.0.10', 'python-utils==3.8.2', 'yfinance==0.2.37', 'soxr==0.3.7', 'llvmlite==0.41.1', 'multipledispatch==1.0.0', 'pyproject-hooks==1.0.0', 'kagglehub==0.2.0', 'fastdownload==0.0.7', 'minikanren==1.0.3', 'hyperopt==0.2.7', 'soupsieve==2.5', 'cryptography==42.0.5', 'parso==0.8.3', 'iniconfig==2.0.0', 'opencv-python-headless==4.9.0.80', 'jeepney==0.7.1', 'pydantic-core==2.16.3', 'google-crc32c==1.5.0', 'tensorboard==2.15.2', 'pyshp==2.3.1', 'annotated-types==0.6.0', 'sympy==1.12', 'websocket-client==1.7.0', 'pyperclip==1.8.2', 'pymystem3==0.2.0', 'sniffio==1.3.1', 'gspread-dataframe==3.3.1', 'ipyleaflet==0.18.2', 'music21==9.1.0', 'keyring==23.5.0', 'peewee==3.17.1', 'jsonpickle==3.0.3', 'appdirs==1.4.4', 'astunparse==1.6.3', 'humanize==4.7.0', 'mdit-py-plugins==0.4.0', 'polars==0.20.2', 'docutils==0.18.1', 'opt-einsum==3.3.0', 'pymc==5.10.4', 'networkx==3.2.1', 'jsonschema-specifications==2023.12.1', 'qudida==0.0.4', 'tzlocal==5.2', 'pyasn1==0.5.1', 'webcolors==1.13', 'cmdstanpy==1.2.1', 'launchpadlib==1.10.16', 'exceptiongroup==1.2.0', 'nibabel==4.0.2', 'google-auth==2.27.0', 'ipytree==0.2.2', 'preshed==3.0.9', 'googledrivedownloader==0.4', 'torchvision==0.17.1+cu121', 'scikit-learn==1.2.2', 'tinycss2==1.2.1', 'scooby==0.9.2', 'ipyfilechooser==0.6.0', 'aiosignal==1.3.1', 'grpcio-status==1.48.2', 'panel==1.3.8', 'pandas-stubs==1.5.3.230304', 'nest-asyncio==1.6.0', 'nltk==3.8.1', 'jinja2==3.1.3', 'mdurl==0.1.2', 'scikit-image==0.19.3', 'torchsummary==1.5.1', 'holoviews==1.17.1', 'imgaug==0.4.0', 'glob2==0.7', 'py4j==0.10.9.7', 'tbb==2021.11.0', 'platformdirs==4.2.0', 'sqlglot==20.11.0', 'threadpoolctl==3.3.0', 'lazy-loader==0.3', 'kaggle==1.5.16', 'bleach==6.1.0', 'xarray-einstats==0.7.0', 'gspread==3.4.2', 'itsdangerous==2.1.2', 'attrs==23.2.0', 'pyarrow==14.0.2', 'torchtext==0.17.1', 'en-core-web-sm==3.7.1', 'rpy2==3.4.2', 'google-colab==1.0.0', 'graphviz==0.20.1', 'requests==2.31.0', 'cachecontrol==0.14.0', 'zict==3.0.0', 'bidict==0.23.1', 'pyyaml==6.0.1', 'entrypoints==0.4', 'python-apt==0.0.0', 'webencodings==0.5.1', 'patsy==0.5.6', 'pygments==2.16.1', 'terminado==0.18.1', 'fastcore==1.5.29', 'mpmath==1.3.0', 'pywavelets==1.5.0', 'pygobject==3.42.1', 'community==1.0.0b1', 'pyparsing==3.1.2', 'gcsfs==2023.6.0', 'anyio==3.7.1', 'librosa==0.10.1', 'google-cloud-storage==2.8.0', 'tomli==2.0.1', 'vega-datasets==0.9.0', 'nbformat==5.10.2', 'kiwisolver==1.4.5', 'tf-keras==2.15.1', 'oauth2client==4.1.3', 'shapely==2.0.3', 'pyasn1-modules==0.3.0', 'proglog==0.1.10', 'requirements-parser==0.5.0', 'textblob==0.17.1', 'google-cloud-resource-manager==1.12.3', 'colour==0.1.5', 'thinc==8.2.3', 'secretstorage==3.3.1', 'plotly==5.15.0', 'lazr.restfulclient==0.14.4', 'easydict==1.13', 'python-box==7.1.1', 'jupyter-client==6.1.12', 'pyviz-comms==3.0.1', 'pathlib==1.0.1', 'joblib==1.3.2', 'linkify-it-py==2.0.3', 'inflect==7.0.0', 'sphinxcontrib-applehelp==1.0.8', 'malloy==2023.1067', 'prometheus-client==0.20.0', 'protobuf==3.20.3', 'tokenizers==0.15.2', 'matplotlib-venn==0.11.10', 'types-setuptools==69.2.0.20240317', 'geographiclib==2.0', 'gdal==3.6.4', 'missingno==0.5.2', 'google-cloud-datastore==2.15.2', 'uritemplate==4.1.1', 'jaxlib==0.4.23+cuda12.cudnn89', 'mistune==0.8.4', 'matplotlib-inline==0.1.6', 'prettytable==3.10.0', 'wcwidth==0.2.13', 'tqdm==4.66.2', 'spacy-legacy==3.0.12', 'sqlparse==0.4.4', 'defusedxml==0.7.1', 'jupyter-console==6.1.0', 'pyerfa==2.0.1.1', 'pytz==2023.4', 'pydantic==2.6.4', 'statsmodels==0.14.1', 'tornado==6.3.3', 'psutil==5.9.5', 'python-dateutil==2.8.2', 'scipy==1.11.4', 'google-auth-oauthlib==1.2.0', 'ibis-framework==8.0.0', 'colorcet==3.1.0', 'ratelim==0.1.6', 'sentencepiece==0.1.99', 'async-timeout==4.0.3', 'holidays==0.44', 'jsonschema==4.19.2', 'pyopenssl==24.1.0', 'pydot==1.4.2', 'argon2-cffi==23.1.0', 'google-pasta==0.2.0', 'numexpr==2.9.0', 'pandas==1.5.3', 'xarray==2023.7.0', 'cycler==0.12.1', 'pandas-gbq==0.19.2', 'sortedcontainers==2.4.0', 'branca==0.7.1', 'cvxopt==1.3.2', 'pytensor==2.18.6', 'build==1.1.1', 'editdistance==0.6.2', 'mizani==0.9.3', 'eerepr==0.0.4', 'more-itertools==10.1.0', 'natsort==8.4.0', 'httpimport==1.3.1', 'pandas-datareader==0.10.0', 'markupsafe==2.1.5', 'google-cloud-firestore==2.11.1', 'pyopengl==3.1.7', 'chardet==5.2.0', 'gast==0.5.4', 'gym-notices==0.0.8', 'prophet==1.1.5', 'tensorflow-estimator==2.15.0', 'pydrive2==1.6.3', 'rpds-py==0.18.0', 'grpc-google-iam-v1==0.13.0', 'h5py==3.9.0', 'markdown==3.6', 'ipyevents==2.0.2', 'pickleshare==0.7.5', 'google-api-python-client==2.84.0', 'wrapt==1.14.1', 'google-cloud-translate==3.11.3', 'sphinxcontrib-devhelp==1.0.6', 'future==0.18.3', 'tensorflow-gcs-config==2.15.0', 'tabulate==0.9.0', 'flax==0.8.2', 'google-cloud-bigquery-connection==1.12.1', 'optax==0.2.1', 'packaging==24.0', 'etils==1.7.0', 'pysocks==1.7.1', 'tf-slim==1.1.0', 'typing-extensions==4.10.0', 'qdldl==0.1.7.post0', 'blosc2==2.0.0', 'spacy==3.7.4', 'cffi==1.16.0', 'google-auth-httplib2==0.1.1', 'werkzeug==3.0.1', 'wordcloud==1.9.3', 'requests-oauthlib==1.4.0', 'imutils==0.5.4', 'distributed==2023.8.1', 'numba==0.58.1', 'geocoder==1.38.1', 'importlib-resources==6.3.0', 'blinker==1.4', 'proto-plus==1.23.0', 'toml==0.10.2', 'imageio==2.31.6', 'xlrd==2.0.1', 'logical-unification==0.4.6', 'py-cpuinfo==9.0.0', 'geemap==0.32.0', 'yellowbrick==1.5', 'psycopg2==2.9.9', 'pexpect==4.9.0', 'plotnine==0.12.4', 'prefetch-generator==1.0.3', 'click-plugins==1.1.1', 'google-cloud-functions==1.13.3', 'cons==0.4.6', 'google-cloud-iam==2.14.3', 'rsa==4.9', 'spacy-loggers==1.0.5', 'pandocfilters==1.5.1', 'cloudpickle==2.2.1', 'audioread==3.0.1', 'pyzmq==23.2.1', 'snowballstemmer==2.2.0', 'libclang==16.0.6', 'keras==2.15.0', 'weasel==0.3.4', 'contourpy==1.2.0', 'referencing==0.33.0', 'urllib3==2.0.7', 'dlib==19.24.2', 'jupyter-server==1.24.0', 'jupyterlab-widgets==3.0.10', 'fonttools==4.49.0', 'certifi==2024.2.2', 'earthengine-api==0.1.394', 'idna==3.6', 'portpicker==1.5.2', 'pygame==2.5.2', 'cloudpathlib==0.16.0', 'firebase-admin==5.3.0', 'text-unidecode==1.3', 'albumentations==1.3.1', 'param==2.0.2', 'regex==2023.12.25', 'lightgbm==4.1.0', 'google-ai-generativelanguage==0.4.0', 'seaborn==0.13.1', 'transformers==4.38.2', 'matplotlib==3.7.1', 'debugpy==1.6.6', 'pycocotools==2.0.7', 'imagesize==1.4.1', 'frozendict==2.4.0', 'backcall==0.2.0', 'send2trash==1.8.2', 'sphinxcontrib-htmlhelp==2.0.5', 'google-cloud-bigquery==3.12.0', 'ecos==2.0.13', 'oauthlib==3.2.2', 'soundfile==0.12.1', 'cachetools==5.3.3', 'googleapis-common-protos==1.63.0', 'frozenlist==1.4.1', 'altair==4.2.2', 'greenlet==3.0.3', 'pluggy==1.4.0', 'click==8.1.7', 'markdown-it-py==3.0.0', 'tensorflow-io-gcs-filesystem==0.36.0', 'fastjsonschema==2.19.1', 'partd==1.4.1', 'nbclassic==1.0.0', 'xyzservices==2023.10.1', 'nbclient==0.10.0', 'gym==0.25.2', 'lazr.uri==1.0.6', 'filelock==3.13.1', 'autograd==1.6.2', 'scs==3.2.4.post1', 'moviepy==1.0.3', 'argon2-cffi-bindings==21.2.0', 'pycparser==2.21', 'prompt-toolkit==3.0.43', 'sqlalchemy==2.0.28', 'setuptools==67.7.2', 'cymem==2.0.8', 'duckdb==0.9.2', 'mkl==2023.2.0', 'zipp==3.18.1', 'fiona==1.9.6', 'arviz==0.15.1', 'rich==13.7.1', 'msgpack==1.0.8', 'traitlets==5.7.1', 'google-api-core==2.11.1', 'multitasking==0.0.11', 'tblib==3.0.0', 'fastrlock==0.8.2', 'aiohttp==3.9.3', 'opencv-python==4.8.0.76', 'confection==0.1.4', 'wheel==0.43.0', 'db-dtypes==1.2.0', 'wasabi==1.1.2', 'wadllib==1.3.6', 'tensorflow-datasets==4.9.4', 'pyjwt==2.3.0', 'six==1.16.0', 'ipython-sql==0.5.0', 'google-cloud-aiplatform==1.44.0', 'pip-tools==6.13.0', 'pytest==7.4.4', 'astropy==5.3.4', 'tensorflow-probability==0.23.0', 'google-generativeai==0.3.2', 'mlxtend==0.22.0', 'beautifulsoup4==4.12.3', 'osqp==0.6.2.post8', 'datascience==0.17.6', 'colorlover==0.3.0', 'yarl==1.9.4', 'smart-open==6.4.0', 'flatbuffers==24.3.7', 'torchaudio==2.2.1+cu121', 'geopy==2.3.0', 'traittypes==0.2.1', 'python-louvain==0.16', 'torchdata==0.7.1', 'chex==0.1.85', 'huggingface-hub==0.20.3', 'pooch==1.8.1', 'jupyter-core==5.7.2', 'tensorflow==2.15.0', 'cython==3.0.9', 'pydotplus==2.0.2', 'bqplot==0.12.43', 'cupy-cuda12x==12.2.0', 'tensorboard-data-server==0.7.2', 'atpublic==4.0', 'ipywidgets==7.7.1', 'tensorflow-metadata==1.14.0', 'sphinxcontrib-jsmath==1.0.1', 'multidict==6.0.5', 'safetensors==0.4.2', 'ipykernel==5.5.6', 'google==2.0.3', 'locket==1.0.0', 'google-cloud-core==2.3.3', 'babel==2.14.0', 'parsy==2.1', 'array-record==0.5.0', 'numpy==1.25.2', 'tifffile==2024.2.12', 'promise==2.3', 'python-slugify==8.0.4', 'stanio==0.3.0', 'distro==1.7.0', 'pyarrow-hotfix==0.6', 'ptyprocess==0.7.0', 'fsspec==2023.6.0', 'httplib2==0.22.0', 'imageio-ffmpeg==0.4.9', 'gensim==4.3.2', 'dm-tree==0.1.8', 'geopandas==0.13.2', 'cufflinks==0.17.3', 'typer==0.9.0', 'fastai==2.7.14', 'cmake==3.27.9', 'dask==2023.8.1', 'tweepy==4.14.0', 'orbax-checkpoint==0.4.4', 'sphinxcontrib-serializinghtml==1.1.10', 'jupyterlab-pygments==0.3.0', 'html5lib==1.1', 'google-cloud-bigquery-storage==2.24.0', 'lxml==4.9.4', 'google-cloud-language==2.13.3', 'opencv-contrib-python==4.8.0.76', 'tensorstore==0.1.45', 'gin-config==0.5.0', 'intel-openmp==2023.2.4', 'pip==23.1.2', 'notebook==6.5.5', 'et-xmlfile==1.1.0', 'pillow==9.4.0', 'sphinx==5.0.2', 'gdown==4.7.3', 'importlib-metadata==7.0.2', 'decorator==4.4.2', 'progressbar2==4.2.0', 'widgetsnbextension==3.6.6', 'termcolor==2.4.0', 'toolz==0.12.1', 'cligj==0.7.2', 'nbconvert==6.5.4', 'notebook-shim==0.2.4', 'sklearn-pandas==2.2.0', 'charset-normalizer==3.3.2', 'alabaster==0.7.16', 'tenacity==8.2.3', 'absl-py==1.4.0', 'pydrive==1.3.1', 'google-resumable-media==2.7.0', 'jieba==0.42.1', 'bokeh==3.3.4', 'ml-dtypes==0.2.0', 'pydot-ng==2.0.0', 'torch==2.2.1+cu121', 'etuples==0.3.9', 'openpyxl==3.1.2', 'xgboost==2.0.3', 'pydata-google-auth==1.8.2', 'jax==0.4.23', 'uc-micro-py==1.0.3', 'flask==2.2.5', 'bigframes==0.25.0', 'h5netcdf==1.3.0'}\n","missing: {'langchain==0.0.214', 'pypdf==3.11.0', 'shapely==1.8.5', 'gradio==3.35.2', 'scann==1.2.9', 'google-cloud-documentai==2.16.0', 'google-cloud-documentai'}\n","installing...\n","done.\n","restarting kernel...\n"]}]},{"cell_type":"markdown","source":["## Initialize environment\n","\n","Note: During Q & A, signed URLs will be served that reference the relevant matches used for summarization by the LLM. As a result, a service account will be used that signs the URLs (this step cannot be performed using user credentials)."],"metadata":{"id":"IggwzaHknI5_"}},{"cell_type":"code","source":["PROJECT_ID = \"\" # @param {type:\"string\"} <---CHANGE THESE\n","%env PROJECT_ID=$PROJECT_ID\n","!gcloud config set project $PROJECT_ID --quiet\n","\n","BUCKET_NAME = \"\" # @param {type:\"string\"} <---CHANGE THESE\n","%env BUCKET_NAME=$BUCKET_NAME\n","\n","BUCKET_URI = f\"gs://{BUCKET_NAME}\"\n","\n","\n","REGION = \"\" # @param {type:\"string\"} <---CHANGE THESE\n","LOCATION = \"\" # @param {type:\"string\"} <---CHANGE THESE\n","\n","USE_SERVICE_ACCOUNT=False #@param {type: \"boolean\"}\n","\n","GCS_URL = \"\" # @param {type:\"string\"} <---CHANGE THESE\n","\n","PROCESSOR_ID = \"\\\"\\\"\" # @param {type:\"string\"}\n","%env PROCESSOR_ID=$PROCESSOR_ID\n","\n","# LLM model\n","MODEL_NAME = \"text-bsion-32k-tuned-model\" #@param {type: \"string\"}\n","max_output_tokens = 8192 #@param {type: \"integer\"}\n","temperature = 0 #@param {type: \"number\"}\n","top_p = 1 #@param {type: \"number\"}\n","top_k = 40 #@param {type: \"number\"}\n","verbose = True #@param {type: \"boolean\"}\n","\n","\n","template_path = \"https://us-kfp.pkg.dev/ml-pipeline/large-language-model-pipelines/tune-large-model/v2.0.0\" #@param {type: \"string\"}\n","!unset GOOGLE_APPLICATION_CREDENTIALS\n","!unset SIGNING_SERVICE_ACCOUNT\n","\n","%reload_ext autoreload\n","%autoreload 2\n"],"metadata":{"id":"mzLNceYG6iuR","executionInfo":{"status":"ok","timestamp":1710880820775,"user_tz":240,"elapsed":1715,"user":{"displayName":"Alex Burdenko","userId":"04128376985135405510"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"ea9cc6e6-4b20-44c4-c67a-a8d8896e8579"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["env: PROJECT_ID=kallogjeri-project-345114\n","Updated property [core/project].\n","env: BUCKET_NAME=medical_text_demo\n","env: PROCESSOR_ID=7d38c27a990b7d9a\n"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"-fDj8AaIBQZs"}},{"cell_type":"code","source":["import sys\n","import os\n","\n","FILE_EXISTS=os.path.exists( '/content/service_account.json' )\n","IN_COLAB = 'google.colab' in sys.modules\n","%env IN_COLAB=$IN_COLAB\n","\n","if USE_SERVICE_ACCOUNT:\n","  if IN_COLAB and not FILE_EXISTS:\n","    from google.colab import files\n","\n","    uploaded = files.upload()\n","\n","    for fn in uploaded.keys():\n","      txt = \"User uploaded file {name} with length {length} bytes\".format( name=fn, length=len(uploaded[fn]))\n","      print( txt )\n","\n","    import os\n","    os.rename(f\"/content/{fn}\",'/content/service_account.json')\n","    GOOGLE_APPLICATION_CREDENTIALS=\"/content/service_account.json\"\n","    %env GOOGLE_APPLICATION_CREDENTIALS=$GOOGLE_APPLICATION_CREDENTIALS\n","\n","\n","    # Grant Service Account Token Creator role on the signing service acocunt\n","    GOOGLE_CLOUD_SERVICE_ACCOUNT=fn.rsplit('.', 1)[0]\n","    %env GOOGLE_CLOUD_SERVICE_ACCOUNT=$GOOGLE_CLOUD_SERVICE_ACCOUNT\n","    !gcloud auth activate-service-account --key-file=$GOOGLE_APPLICATION_CREDENTIALS\n","from google.colab import auth\n","auth.authenticate_user()\n","\n","import google.auth\n","credentials, project = google.auth.default()\n","# Add the cloud-platform scope to the credentials.\n","credentials = credentials.with_scopes([\"https://www.googleapis.com/auth/cloud-platform\"])\n","\n","print(credentials)\n","from google.cloud.bigquery import magics\n","magics.context.credentials = credentials\n","\n","\n"],"metadata":{"id":"zFGBtToxj3oD","executionInfo":{"status":"ok","timestamp":1710880824190,"user_tz":240,"elapsed":1543,"user":{"displayName":"Alex Burdenko","userId":"04128376985135405510"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"ddf0fb57-ea71-43c8-f6e5-d29de63a9a23"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["env: IN_COLAB=True\n","<google.auth.compute_engine.credentials.Credentials object at 0x79e259dee8f0>\n"]}]},{"cell_type":"markdown","source":["## Enable APIs\n","\n","Skip step if APIs are already enabled and user account has __Service Account Token Creator__ role on the signing service account.\n","\n","If APIs are not already enabled, you will be prompted to login again but this time through the _!gcloud auth login_, so gcloud commands can be executed. This step __only__ has to be executed once (if requirements aren't already met) üòÑ"],"metadata":{"id":"OLe38ruNp-QP"}},{"cell_type":"code","source":["#!gcloud auth application-default login -q\n","#!gcloud auth application-default set-quota-project {PROJECT_ID}\n","#!gcloud config set project {PROJECT_ID}\n","\n","# Enable Document AI\n","!gcloud services enable documentai.googleapis.com --quiet\n","\n","# Enable Vertex AI\n","!gcloud services enable aiplatform.googleapis.com --quiet\n","\n","# Enable IAM Credentials API\n","!gcloud services enable iamcredentials.googleapis.com --quiet"],"metadata":{"id":"qusYjPGvo9ft"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Ensure your user account has the ability to sign URLs using the service account."],"metadata":{"id":"t-vEOVPRwGEp"}},{"cell_type":"markdown","source":["## Import packages"],"metadata":{"id":"AbGyl5s7CDf-"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"vrbDu8NzmGqw","executionInfo":{"status":"ok","timestamp":1710880845299,"user_tz":240,"elapsed":11801,"user":{"displayName":"Alex Burdenko","userId":"04128376985135405510"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"9b074676-7ec3-4926-9cff-d4812b713ab8"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/gradio_client/documentation.py:103: UserWarning: Could not get documentation group for <class 'gradio.mix.Parallel'>: No known documentation group for module 'gradio.mix'\n","  warnings.warn(f\"Could not get documentation group for {cls}: {exc}\")\n","/usr/local/lib/python3.10/dist-packages/gradio_client/documentation.py:103: UserWarning: Could not get documentation group for <class 'gradio.mix.Series'>: No known documentation group for module 'gradio.mix'\n","  warnings.warn(f\"Could not get documentation group for {cls}: {exc}\")\n"]}],"source":["import concurrent.futures\n","import datetime\n","import gradio as gr\n","import IPython\n","import numpy as np\n","import os\n","import pandas as pd\n","import PIL, PIL.ImageDraw\n","import requests\n","import scann\n","import shapely\n","import time\n","import tempfile\n","import threading\n","import vertexai\n","from io import BytesIO\n","from google.api_core.client_options import ClientOptions\n","from google.auth import default\n","from google.auth import impersonated_credentials\n","from google.cloud import documentai, storage\n","\n","from langchain.chains.question_answering import load_qa_chain\n","from langchain.docstore.document import Document\n","from langchain.document_loaders.base import BaseLoader\n","from langchain.document_loaders.unstructured import UnstructuredFileLoader\n","from langchain.embeddings.base import Embeddings\n","from langchain.embeddings import VertexAIEmbeddings\n","from langchain.llms import BaseLLM, VertexAI\n","from langchain.prompts import PromptTemplate\n","from langchain.schema import Document\n","from langchain.vectorstores.base import VectorStore\n","from langchain.vectorstores import MatchingEngine\n","from pypdf import PdfReader, PdfWriter\n","from tqdm import tqdm\n","from typing import Any, Iterable, List, Optional, Sequence, Tuple, Type\n","from urllib.parse import urlparse"]},{"cell_type":"markdown","source":["## LangChain wrapper utilities\n","\n","**Run the following cells**\n","\n","* Custom SourceDocument\n","* Custom DocAI DocumentLoader\n","* Custom ScaNN VectorStore\n","* DocumentBot"],"metadata":{"id":"WQC8jQYrwAUV"}},{"cell_type":"markdown","source":["## Instruction Tune Model\n","Now it's time to start to tune a model. You will use the Vertex AI SDK to submit our tuning job.\n","\n","Recommended Tuning Configurations\n","‚úÖ Here are some recommended configurations for tuning a foundation model based on the task, in this example Q&A. You can find more in the documentation.\n","\n","Extractive QA:\n","\n","Make sure that your train dataset size is 100+\n","Training steps [100-500]. You can try more than one value to get the best performance on a particular dataset (e.g. 100, 200, 500)"],"metadata":{"id":"b7LOPBBj4A5R"}},{"cell_type":"code","source":["from google.cloud import storage\n","\n","def save_dict_to_jsonl_in_gcs(dictionary, gcs_bucket_name, gcs_file_name):\n","    \"\"\"Saves a dictionary to a CSV file in a Google Cloud Storage bucket.\n","\n","    Args:\n","        dictionary: The dictionary to be saved.\n","        gcs_bucket_name: The name of the GCS bucket.\n","        gcs_file_name: The name of the CSV file to create.\n","    \"\"\"\n","\n","    storage_client = storage.Client()\n","    bucket = storage_client.bucket(gcs_bucket_name)\n","    blob = bucket.blob(gcs_file_name)\n","\n","    import json\n","    from io import StringIO\n","    buf = StringIO()\n","    for key, value in dictionary.items():\n","      json_line = json.dumps({\"input_text\": key,  \"output_text\" : value})\n","\n","\n","      buf.write(json_line + '\\n')\n","\n","    # Upload the temporary CSV file to GCS\n","    blob.upload_from_string( buf.getvalue() )\n","\n","# Example usage\n","my_dictionary = {'Name': 'Alice', 'Age': 25, 'City': 'New York'}\n","gcs_bucket_name = 'medical_text_demo'\n","gcs_file_name = 'my_data.csv'\n","\n","save_dict_to_jsonl_in_gcs(my_dictionary, BUCKET_NAME, gcs_file_name)"],"metadata":{"id":"V_2rz1J3NM8w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","from google.cloud import documentai_v1 as documentai\n","from google.cloud import storage\n","\n","\n","def process_document_from_gcs(project_id, location, processor_id, gcs_uri):\n","    \"\"\"Processes a document stored in Google Cloud Storage.\"\"\"\n","\n","    # GCS Authentication (assumes credentials setup from environment variable)\n","    storage_client = storage.Client()\n","\n","    # Get bucket and file name from GCS URI\n","    bucket_name, file_path = gcs_uri.replace(\"gs://\", \"\").split(\"/\", 1)\n","    print(f\"bucket_name: {bucket_name}\")\n","    print(f\"gcs_uri: {gcs_uri}\")\n","    print(f\"file_name: {file_path}\")\n","\n","    # Download the file temporarily\n","    # blob = storage_client.bucket(bucket_name).get_blob(gcs_uri)\n","    # print(blob)\n","    # temp_file_path = f'/content/{file_name}'  # Or another suitable local path\n","    # blob.download_to_filename(temp_file_path)\n","\n","\n","    bucket = storage_client.get_bucket(bucket_name)\n","    blob = bucket.blob(file_path)\n","    file_name = file_path.split('/')[1]\n","    temp_file_path = f'/content/{file_name}'  # Or another suitable local path\n","    blob.download_to_filename(temp_file_path)\n","\n","    # Process the file (Identical to previous function)\n","    client = documentai.DocumentProcessorServiceClient()\n","    processor_name = f\"projects/{project_id}/locations/{location}/processors/{processor_id}\"\n","\n","    with open(temp_file_path, \"rb\") as image_file:\n","        image_content = image_file.read()\n","\n","    raw_document = documentai.types.RawDocument(content=image_content, mime_type='application/pdf')\n","    request = documentai.types.ProcessRequest(name=processor_name, raw_document=raw_document)\n","\n","    dict = {}\n","    try:\n","      result = client.process_document(request=request)\n","      document = result.document\n","      for entity in document.entities:\n","        # print(f\"Text: {entity.mention_text}\")\n","        # #print(f\"Text: {entity.text_anchor.content}\\n\")\n","        # print(f\"Label: {entity.type}\\n\")\n","\n","        dict[entity.mention_text] = entity.type\n","    except Exception as e:\n","      print(e)\n","\n","\n","    return dict\n","\n","    # Cleanup - delete the temporary file\n","    os.remove(temp_file_path)\n","\n","\n","import re\n","matches = re.match(r\"gs://(.*?)/(.*)\", GCS_URL)\n","\n","output_array = list()\n","\n","if matches:\n","    bucket, folder_prefix = matches.groups()\n","\n","    #print(f\"Output prefix: {folder_prefix}\" )\n","\n","    # Get List of Document Objects from the Output Bucket\n","    storage_client = storage.Client()\n","    blobs = storage_client.list_blobs(bucket, prefix=folder_prefix)\n","\n","    # Document AI may output multiple JSON files per source file\n","    for blob in blobs:\n","      if blob.name.startswith(folder_prefix) and not blob.name.endswith(\"/\"):\n","        file_path=f\"gs://{BUCKET_NAME}/{blob.name}\"\n","        tags = process_document_from_gcs(PROJECT_ID, LOCATION, PROCESSOR_ID, file_path)\n","        output_array.append(tags)\n","        break\n","\n","    #print(f\"Output array: {output_array}\" )\n","\n","    # for blob in output_blobs:\n","\n","    #   print(f\"File path: {file_path}\" )\n","    #   if not file_path.endswith(\"/\"):\n","    #     tags = process_document_from_gcs(PROJECT_ID, LOCATION, PROCESSOR_ID, file_path)\n","\n","\n","TRAINING_DATA_URI=f\"training_data.jsonl\"\n","EVAUATION_DATA_URI=f\"training_data.jsonl\"\n","\n","save_dict_to_jsonl_in_gcs(tags, BUCKET_NAME, TRAINING_DATA_URI)\n","\n"],"metadata":{"id":"hLKhSP8s_0bP","executionInfo":{"status":"ok","timestamp":1710880855143,"user_tz":240,"elapsed":9193,"user":{"displayName":"Alex Burdenko","userId":"04128376985135405510"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"25d8724d-5b7d-4068-ff0f-8b09eacf46f6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["bucket_name: medical_text_demo\n","gcs_uri: gs://medical_text_demo/pdf/1010853_MR first_Redacted_DL.pdf\n","file_name: pdf/1010853_MR first_Redacted_DL.pdf\n"]}]},{"cell_type":"code","source":["from typing import Union\n","\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","\n","from google.cloud import aiplatform\n","from google.cloud import bigquery\n","from vertexai.language_models import TextGenerationModel\n","\n","\n","# create tensorboard\n","display_name = \"Adapter tuning - \"\n","\n","tensorboard = aiplatform.Tensorboard.create(\n","    display_name='Recursion Model Tuning Job',\n","    project=PROJECT_ID,\n","    location=REGION,\n",")\n","\n","print(tensorboard.display_name)\n","print(tensorboard.resource_name)\n","\n","tensorboard_id = tensorboard.resource_name.split(\"tensorboards/\")[-1]\n","print(tensorboard_id)"],"metadata":{"id":"YV8PZdwR_3t8","executionInfo":{"status":"ok","timestamp":1710880857624,"user_tz":240,"elapsed":2485,"user":{"displayName":"Alex Burdenko","userId":"04128376985135405510"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"6c378178-94c2-4c4e-cf10-1837ee547d50"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:google.cloud.aiplatform.tensorboard.tensorboard_resource:Creating Tensorboard\n","INFO:google.cloud.aiplatform.tensorboard.tensorboard_resource:Create Tensorboard backing LRO: projects/273872083706/locations/us-central1/tensorboards/6267713257578954752/operations/3632444504415404032\n","INFO:google.cloud.aiplatform.tensorboard.tensorboard_resource:Tensorboard created. Resource name: projects/273872083706/locations/us-central1/tensorboards/6267713257578954752\n","INFO:google.cloud.aiplatform.tensorboard.tensorboard_resource:To use this Tensorboard in another session:\n","INFO:google.cloud.aiplatform.tensorboard.tensorboard_resource:tb = aiplatform.Tensorboard('projects/273872083706/locations/us-central1/tensorboards/6267713257578954752')\n"]},{"output_type":"stream","name":"stdout","text":["Recursion Model Tuning Job\n","projects/273872083706/locations/us-central1/tensorboards/6267713257578954752\n","6267713257578954752\n"]}]},{"cell_type":"code","source":["import random\n","import string\n","\n","# Generate a uuid of a specifed length(default=8)\n","def generate_uuid(length: int = 8) -> str:\n","    return \"\".join(random.choices(string.ascii_lowercase + string.digits, k=length))\n","\n","UUID = generate_uuid(length=32) # Set length to 32 characters"],"metadata":{"id":"Q1YG4_MtO-uM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["MODEL_NAME = f\"{MODEL_NAME}-{UUID}\"\n","TRAINING_STEPS = 100"],"metadata":{"id":"5xds9WHWLg4G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset_uri = f\"gs://{BUCKET_NAME}/{TRAINING_DATA_URI}\"\n","\n","pipeline_arguments = {\n","    \"model_display_name\": MODEL_NAME,\n","    \"location\": REGION,\n","    \"large_model_reference\": \"text-bison-32k@002\",\n","    \"project\": PROJECT_ID,\n","    \"train_steps\": TRAINING_STEPS,\n","    \"dataset_uri\": dataset_uri,\n","    \"evaluation_interval\": 20,\n","    \"evaluation_data_uri\": dataset_uri,\n","    \"tensorboard_resource_id\": tensorboard_id\n","}\n","pipeline_root = f\"{BUCKET_NAME}/{MODEL_NAME}\"\n","\n","print(f\"Pipeline root: {pipeline_root}\")"],"metadata":{"id":"Wns3qrImLtKo","executionInfo":{"status":"ok","timestamp":1710880858194,"user_tz":240,"elapsed":5,"user":{"displayName":"Alex Burdenko","userId":"04128376985135405510"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"0b8d60c0-96f3-4896-a962-1fd45db4e52c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Pipeline root: medical_text_demo/text-bsion-32k-tuned-model-j7l20t0d4b0qcq6pay0l2yggqcwe5ynp\n"]}]},{"cell_type":"code","source":["# Function that starts the tuning job\n","def tuned_model(\n","    project_id: str,\n","    location: str,\n","    template_path: str,\n","    model_display_name: str,\n","    pipeline_arguments: str,\n","):\n","    \"\"\"Prompt-tune a new model, based on a prompt-response data.\n","\n","    \"training_data\" can be either the GCS URI of a file formatted in JSONL format\n","    (for example: training_data=f'gs://{bucket}/{filename}.jsonl'), or a pandas\n","    DataFrame. Each training example should be JSONL record with two keys, for\n","    example:\n","      {\n","        \"input_text\": <input prompt>,\n","        \"output_text\": <associated output>\n","      },\n","\n","    Args:\n","      project_id: GCP Project ID, used to initialize aiplatform\n","      location: GCP Region, used to initialize aiplatform\n","      template_path: path to the template\n","      model_display_name: Name for your model.\n","      pipeline_arguments: arguments used during pipeline runtime\n","    \"\"\"\n","\n","    aiplatform.init(project=project_id, location=location)\n","\n","    from google.cloud.aiplatform import PipelineJob\n","\n","    job = PipelineJob(\n","        template_path=template_path,\n","        display_name=model_display_name,\n","        parameter_values=pipeline_arguments,\n","        location=REGION,\n","        pipeline_root=pipeline_root,\n","        enable_caching=True,\n","    )\n","\n","    return job"],"metadata":{"id":"dHce9k8PPJQZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vertexai.init(project=PROJECT_ID, location=REGION)\n","\n","job = tuned_model(PROJECT_ID, REGION, template_path, MODEL_NAME, pipeline_arguments)\n","job.runtime_config.gcs_output_directory = f\"gs://{BUCKET_NAME}/tuned_models/\"\n","job.submit()"],"metadata":{"id":"sOT2oXKXPMmA","executionInfo":{"status":"ok","timestamp":1710880860265,"user_tz":240,"elapsed":1881,"user":{"displayName":"Alex Burdenko","userId":"04128376985135405510"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"619affd5-17d8-4f30-efc5-6eb12e43cef0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n","INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/273872083706/locations/us-central1/pipelineJobs/tune-large-model-20240319204059\n","INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n","INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/273872083706/locations/us-central1/pipelineJobs/tune-large-model-20240319204059')\n","INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n","https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/tune-large-model-20240319204059?project=273872083706\n"]}]},{"cell_type":"code","source":["job.state"],"metadata":{"id":"En6pnmhNPjhm","executionInfo":{"status":"ok","timestamp":1710880861057,"user_tz":240,"elapsed":797,"user":{"displayName":"Alex Burdenko","userId":"04128376985135405510"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"656f5bc8-5755-46e8-f07c-79bf1882515d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<PipelineState.PIPELINE_STATE_PENDING: 2>"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["from google.cloud import aiplatform_v1beta1 as aiplatform\n","def fetch_model(project_id, location):\n","    # Initialize the AIPlatform v1beta1 client (Vertex AI)\n","    client_options = {\"api_endpoint\": f\"{location}-aiplatform.googleapis.com\"}\n","    client = aiplatform.ModelServiceClient(client_options=client_options)\n","\n","    # Construct the parent resource name\n","    parent = f\"projects/{project_id}/locations/{location}\"\n","\n","    # Define a filter to list only tuned models (Modify the filter if needed)\n","    filter = 'labels.google-vertex-llm-tuning-base-model-id:*'\n","\n","    # Prepare the request\n","    request = aiplatform.ListModelsRequest(parent=parent, filter=filter)\n","\n","    # Call the API to list models\n","    response = client.list_models(request=request)\n","    tuned_model = response.models[0]\n","\n","    return tuned_model"],"metadata":{"id":"lYP0IIb-P5VE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["deployed_model = fetch_model(PROJECT_ID, REGION)\n","\n","print(deployed_model.display_name)\n","print(deployed_model)\n","\n","#deployed_model = TextGenerationModel.get_tuned_model(deployed_model)"],"metadata":{"id":"9seDQ9kC7n4M","executionInfo":{"status":"ok","timestamp":1710880862293,"user_tz":240,"elapsed":1045,"user":{"displayName":"Alex Burdenko","userId":"04128376985135405510"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"9a03cb2b-7238-4a58-b20f-18a5190898b7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["genai-workshop-tuned-model-olp45jum\n","name: \"projects/273872083706/locations/us-central1/models/9000261336369725440\"\n","display_name: \"genai-workshop-tuned-model-olp45jum\"\n","predict_schemata {\n","  instance_schema_uri: \"https://storage.googleapis.com/google-cloud-aiplatform/schema/predict/instance/text_generation_1.0.0.yaml\"\n","  parameters_schema_uri: \"https://storage.googleapis.com/google-cloud-aiplatform/schema/predict/params/text_generation_1.0.0.yaml\"\n","  prediction_schema_uri: \"https://storage.googleapis.com/google-cloud-aiplatform/schema/predict/prediction/text_generation_1.0.0.yaml\"\n","}\n","create_time {\n","  seconds: 1688192080\n","  nanos: 144464000\n","}\n","update_time {\n","  seconds: 1688192082\n","  nanos: 244416000\n","}\n","deployed_models {\n","  endpoint: \"projects/273872083706/locations/us-central1/endpoints/2858162884217667584\"\n","  deployed_model_id: \"3757640361552379904\"\n","}\n","etag: \"AMEw9yMcFmaSgxzjp8iBMB7LJhaBwwCVoBQf1t1Iq8bQ3GUrZffNkPbQw2pb0UHmfDQ=\"\n","labels {\n","  key: \"google-vertex-llm-tuning-base-model-id\"\n","  value: \"text-bison-001\"\n","}\n","version_id: \"1\"\n","version_aliases: \"default\"\n","version_create_time {\n","  seconds: 1688192080\n","  nanos: 144464000\n","}\n","version_update_time {\n","  seconds: 1688192082\n","  nanos: 244416000\n","}\n","model_source_info {\n","  source_type: GENIE\n","}\n","base_model_source {\n","  genie_source {\n","  }\n","}\n","\n"]}]},{"cell_type":"code","source":["from langchain.llms import BaseLLM, VertexAI\n","\n","# Construct the tuned model resource name\n","model_name=deployed_model.name\n","\n","llm = VertexAI(\n","  tuned_model_name=model_name,\n","  max_output_tokens=max_output_tokens,\n","  temperature=temperature,\n","  top_p=top_p,\n","  top_k=top_k,\n","  verbose=verbose\n",")\n","\n","\n"],"metadata":{"id":"z8k5cRu1Wxg1"},"execution_count":null,"outputs":[]}]}