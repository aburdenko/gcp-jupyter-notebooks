{"cells":[{"cell_type":"markdown","metadata":{"id":"3JHIWNPudBCG"},"source":["#Install Packages"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":150,"status":"ok","timestamp":1680623268703,"user":{"displayName":"Alex Burdenko","userId":"04128376985135405510"},"user_tz":240},"id":"wVWSsvhZ8kPF","outputId":"6167d17b-024b-4792-c246-a1453b15cb1c"},"outputs":[{"output_type":"stream","name":"stdout","text":["working set entries: <pkg_resources.WorkingSet object at 0x7f0f90f45070>\n","adding kaggle==1.5.13\n","adding google-cloud-aiplatform==1.23.0\n","adding smart-open==6.3.0\n","adding ipdb==0.13.13\n","missing: set()\n"]}],"source":["import sys\n","import subprocess\n","import pkg_resources\n","#!pip install -Uqq ipdb\n","\n","\n","required = {\n","      'kaggle'\n","      , 'google-cloud-aiplatform'\n","      , 'smart-open'\n","      , 'ipdb'\n","    }\n","\n","print(f\"working set entries: {pkg_resources.working_set}\")\n","\n","specific_required = set()\n","\n","for pkg in required:\n","  if not '==' in pkg and pkg in pkg_resources.working_set.by_key.keys():\n","    str_key=str( pkg_resources.get_distribution(pkg) ).replace(' ','==')\n","    print(f\"adding {str_key}\")\n","    specific_required.add(str_key)\n","  else:\n","    specific_required.add(pkg)\n","\n","required=specific_required\n","\n","required.discard(None)\n","installed = { f\"{pkg.key}=={pkg_resources.get_distribution(pkg.key).version}\" for pkg in pkg_resources.working_set}\n","missing = required - installed\n","print(f\"missing: {missing}\")\n","if missing:\n","    python = sys.executable\n","    #subprocess.check_call([python, '-m', 'pip', 'install', '--upgrade', f\"--target={PYTHON_LIB_PATH}\", *missing], stdout=subprocess.DEVNULL)\n","    subprocess.check_call([python, '-m', 'pip', 'install', '--upgrade', *missing], stdout=subprocess.DEVNULL)\n","\n","    import IPython\n","    import time\n","    print( 'restarting kernel...' )\n","    app = IPython.Application.instance()\n","    app.kernel.do_shutdown(True)\n","\n","    # Wait for the kernel to \"crash\". Haven't found a more elegant workaround for do_shutdown being an async call..."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1228,"status":"ok","timestamp":1680623272497,"user":{"displayName":"Alex Burdenko","userId":"04128376985135405510"},"user_tz":240},"id":"C2GSwMxu3n9y","outputId":"cbed3ad7-ec0c-4233-cfdb-4c7a81fa249f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Authenticated\n"]}],"source":["import sys\n","if 'google.colab' in sys.modules:\n","  from google.colab import auth\n","  auth.authenticate_user()\n","  print('Authenticated')\n","\n","\n","import google.auth\n","credentials, project = google.auth.default()\n","from google.cloud.bigquery import magics\n","magics.context.credentials = credentials"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2519,"status":"ok","timestamp":1680623276681,"user":{"displayName":"Alex Burdenko","userId":"04128376985135405510"},"user_tz":240},"id":"080Su_zoay0F","outputId":"acc50090-8c80-4c9a-9c38-2f3dfae75ae8"},"outputs":[{"output_type":"stream","name":"stdout","text":["env: IN_COLAB=True\n","env: KAGGLE_DATASET_USERNAME=KAGGLE_DATASET_USERNAME\n","env: KAGGLE_DATASET_NAME=KAGGLE_DATASET_NAME\n","env: GOOGLE_APPLICATION_CREDENTIALS=/content/service_account.json\n","env: BUCKET_NAME=medical_text_demo\n","Updated property [core/project].\n"]}],"source":["PROJECT_ID = \"kallogjeri-project-345114\" # @param {type:\"string\"} <---CHANGE THESE\n","KAGGLE_DATASET_USERNAME = \"sorkun\" # @param {type:\"string\"} <---CHANGE THESE\n","KAGGLE_DATASET_NAME = \"aqsoldb-a-curated-aqueous-solubility-dataset\" # @param {type:\"string\"} <---CHANGE THESE\n","BUCKET_NAME = \"medical_text_demo\" # @param {type:\"string\"} <---CHANGE THESE\n","GOOGLE_APPLICATION_CREDENTIALS=\"/content/service_account.json\"\n","WORKSPACE_HOME=\"/content\" # @param {type:\"string\"} <---CHANGE THESE\n","\n","import sys\n","IN_COLAB = 'google.colab' in sys.modules\n","%env IN_COLAB=$IN_COLAB\n","\n","%env KAGGLE_DATASET_USERNAME = KAGGLE_DATASET_USERNAME\n","%env KAGGLE_DATASET_NAME = KAGGLE_DATASET_NAME\n","%env GOOGLE_APPLICATION_CREDENTIALS=$GOOGLE_APPLICATION_CREDENTIALS\n","%env BUCKET_NAME=$BUCKET_NAME\n","!gcloud config set project $PROJECT_ID -q"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":109},"executionInfo":{"elapsed":3840,"status":"ok","timestamp":1680623280673,"user":{"displayName":"Alex Burdenko","userId":"04128376985135405510"},"user_tz":240},"id":"-0zN7pX7r9cp","outputId":"c994c88a-f1cb-42d6-8c07-03d6050bb991"},"outputs":[{"output_type":"stream","name":"stdout","text":["env: IN_COLAB=True\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-62b6d5b9-e846-489c-9fbf-903de37f39d3\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-62b6d5b9-e846-489c-9fbf-903de37f39d3\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving kallogjeri-project-345114@appspot.gserviceaccount.com.json to kallogjeri-project-345114@appspot.gserviceaccount.com.json\n","User uploaded file kallogjeri-project-345114@appspot.gserviceaccount.com.json with length 2336 bytes\n"]}],"source":["import sys\n","import os\n","\n","FILE_EXISTS=os.path.exists( '/content/service_account.json' )\n","IN_COLAB = 'google.colab' in sys.modules\n","%env IN_COLAB=$IN_COLAB\n","\n","if IN_COLAB and not FILE_EXISTS:\n","  from google.colab import files\n","\n","  uploaded = files.upload()\n","\n","  for fn in uploaded.keys():\n","    txt = \"User uploaded file {name} with length {length} bytes\".format( name=fn, length=len(uploaded[fn]))\n","    print( txt )\n","\n","  import os\n","  os.rename(f\"/content/{fn}\",'/content/service_account.json')"]},{"cell_type":"markdown","metadata":{"id":"vC6LOPvgGXt0"},"source":["Upload Kaggle .json file"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":109},"executionInfo":{"elapsed":6395,"status":"ok","timestamp":1680623289122,"user":{"displayName":"Alex Burdenko","userId":"04128376985135405510"},"user_tz":240},"id":"eFRjirpeGXF9","outputId":"e402ad4b-cc57-42f4-e0fe-b00230961e2f"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-328d0786-0453-4363-87b4-cffca78c476e\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-328d0786-0453-4363-87b4-cffca78c476e\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving kaggle.json to kaggle.json\n","User uploaded file kaggle.json with length 68 bytes\n","{\"username\":\"alexburdenko\",\"key\":\"bdc6f69f4aa714222d8c52fbf1aeca28\"}"]}],"source":["import sys\n","import os\n","\n","FILE_EXISTS=os.path.exists( '/root/.kaggle/kaggle.json' )\n","if IN_COLAB and not FILE_EXISTS:\n","\n","  !mkdir -p /root/.kaggle\n","\n","  from google.colab import files\n","\n","  uploaded = files.upload()\n","\n","  for fn in uploaded.keys():\n","    txt = \"User uploaded file {name} with length {length} bytes\".format( name=fn, length=len(uploaded[fn]))\n","    print( txt )\n","\n","  import os\n","  os.rename(f\"{fn}\",'/root/.kaggle/kaggle.json')\n","  !chmod 600 /root/.kaggle/kaggle.json\n","\n","  # import json\n","  # token = {\"username\":\"alexburdenko\",\"key\":\"3eb670cc106e7b8cfe41dfeaab51e3a2\"}\n","  # with open('/root/.kaggle/kaggle.json', 'w') as file:\n","  #   json.dump(token, file)\n","\n","\n","!cat $HOME/.kaggle/kaggle.json\n","\n","# %env KAGGLE_USERNAME = 'alexburdenko'\n","# %env KAGGLE_KEY = '0503be621facfe2634edfc421af8d8e0'\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1031,"status":"ok","timestamp":1680623294598,"user":{"displayName":"Alex Burdenko","userId":"04128376985135405510"},"user_tz":240},"id":"avVK12FiT_ru","outputId":"a2f5629c-4219-4635-9a50-7d927f93cbc2"},"outputs":[{"output_type":"stream","name":"stdout","text":["ref                                                                                deadline             category             reward  teamCount  userHasEntered  \n","---------------------------------------------------------------------------------  -------------------  ---------------  ----------  ---------  --------------  \n","https://www.kaggle.com/competitions/vesuvius-challenge-ink-detection               2023-06-14 23:59:00  Featured         $1,000,000        304           False  \n","https://www.kaggle.com/competitions/asl-signs                                      2023-05-01 23:59:00  Research           $100,000        840           False  \n","https://www.kaggle.com/competitions/tlvmc-parkinsons-freezing-gait-prediction      2023-06-08 23:59:00  Research           $100,000        348           False  \n","https://www.kaggle.com/competitions/amp-parkinsons-disease-progression-prediction  2023-05-18 23:59:00  Featured            $60,000        966           False  \n","https://www.kaggle.com/competitions/lux-ai-season-2                                2023-04-24 23:59:00  Featured            $55,000        559           False  \n","https://www.kaggle.com/competitions/predict-student-performance-from-game-play     2023-06-07 23:59:00  Featured            $55,000       1197           False  \n","https://www.kaggle.com/competitions/icecube-neutrinos-in-deep-ice                  2023-04-19 23:59:00  Research            $50,000        709           False  \n","https://www.kaggle.com/competitions/stable-diffusion-image-to-prompts              2023-05-15 23:59:00  Featured            $50,000        850           False  \n","https://www.kaggle.com/competitions/birdclef-2023                                  2023-05-24 23:59:00  Research            $50,000        595           False  \n","https://www.kaggle.com/competitions/benetech-making-graphs-accessible              2023-06-12 23:59:00  Featured            $50,000         81           False  \n","https://www.kaggle.com/competitions/playground-series-s3e12                        2023-04-17 23:59:00  Playground             Swag        127           False  \n","https://www.kaggle.com/competitions/fathomnet-out-of-sample-detection              2023-05-23 23:59:00  Research          Knowledge          4           False  \n","https://www.kaggle.com/competitions/titanic                                        2030-01-01 00:00:00  Getting Started   Knowledge      15513            True  \n","https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques    2030-01-01 00:00:00  Getting Started   Knowledge       4343           False  \n","https://www.kaggle.com/competitions/spaceship-titanic                              2030-01-01 00:00:00  Getting Started   Knowledge       2375           False  \n","https://www.kaggle.com/competitions/digit-recognizer                               2030-01-01 00:00:00  Getting Started   Knowledge       1422           False  \n","https://www.kaggle.com/competitions/nlp-getting-started                            2030-01-01 00:00:00  Getting Started   Knowledge       1122           False  \n","https://www.kaggle.com/competitions/connectx                                       2030-01-01 00:00:00  Getting Started   Knowledge        241           False  \n","https://www.kaggle.com/competitions/tpu-getting-started                            2030-06-03 23:59:00  Getting Started   Knowledge        181           False  \n","https://www.kaggle.com/competitions/store-sales-time-series-forecasting            2030-06-30 23:59:00  Getting Started   Knowledge        706           False  \n"]}],"source":["!kaggle competitions list"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":694,"status":"ok","timestamp":1680623295289,"user":{"displayName":"Alex Burdenko","userId":"04128376985135405510"},"user_tz":240},"id":"dud_TlMvHyYl","outputId":"d41b9a97-c606-4ac9-b32f-0e4652ed94a9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading aqsoldb-a-curated-aqueous-solubility-dataset.zip to /content\n","\r  0% 0.00/1.39M [00:00<?, ?B/s]\n","\r100% 1.39M/1.39M [00:00<00:00, 66.3MB/s]\n"]}],"source":["!kaggle datasets download $KAGGLE_DATASET_USERNAME/$KAGGLE_DATASET_NAME --force"]},{"cell_type":"markdown","metadata":{"id":"aGSWDO_RovH2"},"source":["# Create Vertex Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8459,"status":"ok","timestamp":1680623303744,"user":{"displayName":"Alex Burdenko","userId":"04128376985135405510"},"user_tz":240},"id":"aX5Eu0jAXhZe","outputId":"f8857004-4377-4051-df2c-959acc3187c8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Copying file:///content/aqsoldb-a-curated-aqueous-solubility-dataset.zip [Content-Type=application/zip]...\n","\\\n","Operation completed over 1 objects/1.4 MiB.                                      \n","   1454326  2023-04-04T15:48:20Z  gs://medical_text_demo/aqsoldb-a-curated-aqueous-solubility-dataset.zip#1680623300803909  metageneration=1\n","                                 gs://medical_text_demo/bq_import/\n","                                 gs://medical_text_demo/docai_processed/\n","                                 gs://medical_text_demo/hcls_nl_json/\n","                                 gs://medical_text_demo/jsonl/\n","                                 gs://medical_text_demo/medical-text.zip/\n","                                 gs://medical_text_demo/pdf/\n","                                 gs://medical_text_demo/raw_text/\n","                                 gs://medical_text_demo/train/\n","TOTAL: 1 objects, 1454326 bytes (1.39 MiB)\n"]}],"source":["!gsutil -m cp -r /content/{KAGGLE_DATASET_NAME}.zip gs://{BUCKET_NAME}\n","!gsutil ls -al gs://{BUCKET_NAME}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UNtZrQ-eTKZO"},"outputs":[],"source":["from google.cloud import storage\n","from zipfile import ZipFile\n","from zipfile import is_zipfile\n","import io\n","\n","def zipextract(bucketname, zipfilename_with_path):\n","\n","    storage_client = storage.Client()\n","    bucket = storage_client.get_bucket(bucketname)\n","\n","    destination_blob_pathname = zipfilename_with_path\n","\n","    blob = bucket.blob(destination_blob_pathname)\n","    zipbytes = io.BytesIO(blob.download_as_string())\n","\n","    if is_zipfile(zipbytes):\n","        with ZipFile(zipbytes, 'r') as myzip:\n","            for contentfilename in myzip.namelist():\n","                contentfile = myzip.read(contentfilename)\n","                blob2 = bucket.blob(zipfilename_with_path + \"/\" + contentfilename)\n","                blob2.upload_from_string(contentfile)\n","\n","    blob.delete()\n","\n","filename = f\"{KAGGLE_DATASET_NAME}.zip\"\n","zipextract( BUCKET_NAME, filename ) # if the file is gs://mybucket/path/file.zip"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":684,"status":"ok","timestamp":1680623306418,"user":{"displayName":"Alex Burdenko","userId":"04128376985135405510"},"user_tz":240},"id":"QxUozZVxVsdN","outputId":"9e386353-04ce-4b02-a636-8b9100d20171"},"outputs":[{"output_type":"stream","name":"stdout","text":["uri : gs://medical_text_demo/aqsoldb-a-curated-aqueous-solubility-dataset.zip\n","Output bucket: medical_text_demo\n","Output prefix: aqsoldb-a-curated-aqueous-solubility-dataset.zip\n","jsonl/aqsoldb-a-curated-aqueous-solubility-dataset.zip/curated-solubility-dataset.csv.jsonl\n"]}],"source":["uri=f\"gs://{BUCKET_NAME}/{KAGGLE_DATASET_NAME}.zip\"\n","\n","import re\n","matches = re.match(r\"gs://(.*?)/(.*)\", uri)\n","\n","bucket_name = None\n","folder_path = None\n","if matches:\n","  bucket_name, folder_path = matches.groups()\n","\n","\n","print(f\"uri : {uri}\")\n","print(f\"Output bucket: {bucket_name}\" )\n","print(f\"Output prefix: {folder_path}\" )\n","\n","from google.cloud import storage\n","storage_client = storage.Client()\n","blobs = list(storage_client.list_blobs(bucket_name, prefix=f\"{folder_path}/\", fields=\"items(name)\"))\n","\n","raw_text_file_path = ''\n","hcls_nl_json_uri = ''\n","file_list = list()\n","file_prefixes = list()\n","file_map = dict()\n","for blob in blobs:\n","\n","  if not blob.name.endswith('/'):\n","    # Download JSON File as bytes object and convert to Document Object\n","    file_path=f\"gs://{BUCKET_NAME}/{blob.name}\"\n","    file_prefixes.append(f\"jsonl/{blob.name}.jsonl\")\n","    print(f\"jsonl/{blob.name}.jsonl\")\n","    file_list.append( file_path )\n","\n","    file_map[blob.name] = f\"jsonl/{blob.name}.jsonl\"\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VxO6hl2W3fm_"},"outputs":[],"source":["def to_jsonl( annotations : list):\n","  # import json\n","  # jsonl_output = \"\"\n","  # for annotation in annotations:\n","  #   jsonl_output += json.dumps(annotation) + \"\\n\"\n","  # return jsonl_output\n","\n","  import json\n","  from io import StringIO\n","\n","  res = [json.dumps(annotation) + \"\\n\" for annotation in annotations]\n","  buf = StringIO()\n","  for i in res:\n","    buf.write(i)\n","      #buf.write(i+'\\n')\n","\n","  sbuf = buf.getvalue().replace('\"-','\\\"-')\n","  sbuf = buf.getvalue().replace(\"'-\",\"-\")\n","  sbuf = buf.getvalue().replace(\"'\",\"\\\"\")\n","  sbuf = buf.getvalue().replace(\"#\",\"\\#\")\n","\n","  print(f\"Created import file based on {len(annotations)} annotations\")\n","  return sbuf\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wL05rGwG7HQA"},"outputs":[],"source":["def upload_str_to_bucket(str_buf, bucket_name, file_path = '/' ):\n","    print(\"Attempting to upload str to bucket {} and path {}\".format( bucket_name, file_path))\n","\n","    client = storage.Client()\n","    bucket = client.get_bucket(bucket_name)\n","    blob: Blob = bucket.blob(file_path)\n","    blob.cache_control = \"no-cache,max-age=0\"\n","    blob.content_encoding = \"utf-16\"\n","\n","    blob.upload_from_string( str_buf )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r-9kS3uPLyti"},"outputs":[],"source":["# def get_text_annotations( full_text: str  ) -> list:\n","#   annotations = []\n","#   text_segment_annotations = []\n","#   annotation = {\n","#         #\"textSegmentAnnotations\": text_segment_annotations,\n","#         \"textContent\": full_text\n","#     }\n","\n","#   annotations.append(annotation)\n","#   return annotations\n"]},{"cell_type":"code","source":["def get_text_annotations( resp )->list:\n","  #print(resp)\n","  entity_keys = list()\n","  automl_entities = list()\n","  for entity_mention in resp.get('entityMentions', []):\n","      entity_text = entity_mention['text'].get('content', '') # Insulin regimen human\n","      entity_type = entity_mention['type']\n","\n","      start_offset = entity_mention['text'].get('beginOffset', -1)\n","\n","      ln = len(entity_text)\n","      end_offset = start_offset + ln\n","\n","      #highlight_text = raw_text[start_offset:end_offset]\n","\n","      #print(f\"highlight_text: {highlight_text}\")\n","      # print(f\"entity_type: {entity_type}\")\n","      # print(f\"entity_text: {entity_text}\")\n","\n","      if (start_offset, end_offset) not in entity_keys:\n","\n","          if start_offset > 0 :\n","              automl_entity=dict()\n","              automl_entity['endOffset']=end_offset\n","              automl_entity['startOffset']=start_offset\n","              automl_entity['displayName']=entity_type\n","              #print(automl_entity)\n","              #print(len(automl_entity))\n","              automl_entities.append( automl_entity )\n","              print(f\"Start offset: {start_offset}\")\n","              print(f\"End offset: {end_offset}\")\n","              entity_keys.append( (start_offset, end_offset) )\n","\n","\n","\n","      return automl_entities"],"metadata":{"id":"4zkwU5BAAdhE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.cloud import _http\n","class Connection(_http.JSONConnection):\n","  \"\"\"Handles HTTP requests to GCP.\"\"\"\n","  API_BASE_URL = 'https://healthcare.googleapis.com'\n","  API_VERSION = 'v1beta1'\n","  #API_VERSION = 'v1'\n","  API_URL_TEMPLATE = '{api_base_url}/{api_version}/projects{path}'\n","\n","from google.cloud.client import ClientWithProject\n","class Client(ClientWithProject):\n","  \"\"\"A client for accessing Cloud Healthcare NLP API.\n","\n","  Args:\n","      project (Union[str, None]): The ID of the project\n","      region (str): The region the project resides in, e.g. us-central1,\n","  \"\"\"\n","\n","  def __init__(self,\n","               project,\n","               region='us-central1',\n","               credentials=None,\n","               http=None):\n","    self.region = region\n","    self.SCOPE = ('https://www.googleapis.com/auth/cloud-healthcare',)\n","    super(Client, self).__init__(project=project)\n","    self.path = '/{}/locations/{}/services/nlp'.format(self.project,\n","                                                       self.region)\n","    # self._credentials = credentials\n","    self.project=project\n","    self._connection = Connection(self)\n","\n","\n","  def analyze_entities(self, document):\n","    \"\"\" Analyze the clinical entities a document with the Google Cloud\n","\n","      Healthcare NLP API.\n","\n","      Args:\n","        document (str): the medical document to analyze.\n","\n","      Returns:\n","        Dict[str, Any]: the JSON response.\n","      \"\"\"\n","\n","    # client = language.LanguageServiceClient()\n","    # document = language.Document(content=document, type_=language.Document.Type.PLAIN_TEXT)\n","\n","\n","    # #document = language.Document(content=document, type = language.Document.Type.PLAIN_TEXT)\n","    # #document = language.Document(content=document, type_=language.Document.Type.PLAIN_TEXT)\n","\n","    # client = language.LanguageServiceClient()\n","    # response = client.analyze_entities(document=document)\n","\n","\n","    # print( document )\n","    document = str( document.encode('utf-8') )\n","\n","    return self._connection.api_request(\n","        'POST',\n","        self.path + ':analyzeEntities',\n","        data={'document_content': document})\n","\n","def get_one_mb_str_blocks( str_item: str, blocks: list ):\n","  #MAX_LEN=1000000\n","  MAX_LEN=20000\n","  str_len = len( str_item.encode('utf-16') )\n","\n","  if str_len > MAX_LEN :\n","     rest = str_item[MAX_LEN+1:]\n","\n","     if len(rest.encode('utf-16')) > MAX_LEN:\n","        get_one_mb_str_blocks( rest, blocks )\n","     else:\n","        blocks.append( rest )\n","\n","\n","  else:\n","      blocks.append( str_item )\n","\n","def analyze_entities(str_item, project, creds)->dict:\n","   #API Limits the size of the text field to 1 MB\n","  str_blocks = list()\n","  #str_item = str_item.encode(encoding='utf-8')\n","  get_one_mb_str_blocks( str_item, str_blocks )\n","\n","  client = Client( project=project, credentials=creds )\n","  if len(str_blocks) > 1:\n","    last_fragment = str_blocks[len(str_blocks)-1]\n","\n","  res = dict()\n","\n","  for block in str_blocks:\n","\n","    # str_item = \"Sepsis results in unfettered inflammation, tissue damage, and multiple organ failure. Diffuse brain dysfunction and neurological manifestations secondary to sepsis are termed sepsis-associated encephalopathy (SAE). Extracellular nucleotides, proinflammatory cytokines, and oxidative stress reactions are associated with delirium and brain injury, and might be linked to the pathophysiology of SAE. P2X7 receptor activation by extracellular ATP leads to maturation and release of IL-1Î² by immune cells, which stimulates the production of oxygen reactive species. Hence, we sought to investigate the role of purinergic signaling by P2X7 in a model of sepsis. We also determined how this process is regulated by the ectonucleotidase CD39, a scavenger of extracellular nucleotides. Wild type (WT), P2X7 receptor (P2X7)\"\n","    res.update( client.analyze_entities( block ) )\n","\n","  return res"],"metadata":{"id":"Qq8PATI_WZ5s"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IYnj-g1L23-q","executionInfo":{"status":"ok","timestamp":1680623311753,"user_tz":240,"elapsed":5339,"user":{"displayName":"Alex Burdenko","userId":"04128376985135405510"}},"outputId":"aafc701f-f73d-4638-fcc2-62312f080b68"},"outputs":[{"output_type":"stream","name":"stdout","text":["Automatic pdb calling has been turned OFF\n","medical_text_demo\n","aqsoldb-a-curated-aqueous-solubility-dataset.zip/curated-solubility-dataset.csv\n","/content/curated-solubility-dataset.csv_metadata.pickle\n","checking if aqsoldb-a-curated-aqueous-solubility-dataset.zip/curated-solubility-dataset.csv exists...\n","True\n","downloading file...gs://medical_text_demo/aqsoldb-a-curated-aqueous-solubility-dataset.zip/curated-solubility-dataset.csv\n","Created import file based on 1 annotations\n","Uploading jsonl file jsonl/aqsoldb-a-curated-aqueous-solubility-dataset.zip/curated-solubility-dataset.csv.jsonl\n","Attempting to upload str to bucket medical_text_demo and path jsonl/aqsoldb-a-curated-aqueous-solubility-dataset.zip/curated-solubility-dataset.csv.jsonl\n"]}],"source":["%pdb off\n","import ipdb\n","def download_str_from_bucket(bucket_name, file_path = '/default.csv' )->str:\n","    from google.cloud import storage\n","    import os\n","    import pandas as pd\n","\n","    client = storage.Client()\n","    bucket = client.get_bucket(bucket_name)\n","    print(f\"checking if {file_path} exists...\")\n","    blob = bucket.blob(file_path)\n","    exists = blob.exists( client )\n","    print( exists )\n","\n","    from smart_open import open\n","    from io import StringIO\n","    content_buf = StringIO()\n","\n","\n","    content = None\n","    if exists:\n","      blob_path = f\"gs://{bucket_name}/{file_path}\"\n","      print(f\"downloading file...{blob_path}\")\n","\n","      for line in open(blob_path, encoding=\"ISO-8859-1\"):\n","        content_buf.write( line )\n","\n","      content_buf.flush()\n","      content = content_buf.getvalue()\n","\n","    return content\n","\n","import pprint\n","\n","import pickle\n","from os.path import exists\n","\n","#file_prefixes = [f\"gs://{BUCKET_NAME}/jsonl/{prefix}\" for prefix in file_prefixes]\n","for path_prefix in file_map.keys():\n","  print(bucket_name)\n","  print(path_prefix)\n","\n","  file_name = path_prefix.split('/')[-1]\n","  file_path = f\"{WORKSPACE_HOME}/{file_name}_metadata.pickle\"\n","  print(file_path)\n","\n","  if not exists(file_path):\n","\n","      content = download_str_from_bucket( bucket_name=bucket_name, file_path=path_prefix )\n","      with open(file_path, 'wb') as f:\n","          pickle.dump(content, f)\n","  else:\n","      with open(file_path, 'rb') as handle:\n","          content = pickle.load(handle)\n","\n","\n","  from google.oauth2 import service_account\n","  creds = service_account.Credentials.from_service_account_file( GOOGLE_APPLICATION_CREDENTIALS )\n","\n","  scoped_credentials = creds.with_scopes(\n","      ['https://www.googleapis.com/auth/cloud-platform'])\n","\n","\n","  res = analyze_entities( str_item=content, creds=creds, project=PROJECT_ID )\n","\n","\n","\n","\n","  #text_entities = list()\n","  #text_annotations = dict()\n","  #text_annotations['text_segment_annotations'] =  self._get_text_annotations( res )\n","  #text_annotations['textContent'] = content\n","  #text_entities.append( text_annotations )\n","\n","  text_entities = list()\n","  text_annotations = dict()\n","  text_annotations['textSegmentAnnotations'] = get_text_annotations( res )\n","  text_annotations['textContent'] = content\n","  text_entities.append( text_annotations )\n","\n","  # from pprint import pprint\n","  # pprint(text_annotations)\n","\n","\n","  jsonl = to_jsonl( text_entities )\n","  print(f\"Uploading jsonl file {file_map[path_prefix]}\")\n","\n","# #  ipdb.set_trace(context=6)\n","\n","  upload_str_to_bucket(f\"{jsonl}\"\n","                      , bucket_name=BUCKET_NAME\n","                      , file_path=f\"{file_map[path_prefix]}\" )\n","\n"]},{"cell_type":"markdown","source":["###TODO: create a jsonl file with the CSV data"],"metadata":{"id":"tr9kJy6C4CoG"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qj_QNeHaYEjV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1680623714736,"user_tz":240,"elapsed":402991,"user":{"displayName":"Alex Burdenko","userId":"04128376985135405510"}},"outputId":"11dfd171-6f2e-4fd8-af2e-9c5632748d53"},"outputs":[{"output_type":"stream","name":"stdout","text":["Creating TextDataset\n"]},{"output_type":"stream","name":"stderr","text":["INFO:google.cloud.aiplatform.datasets.dataset:Creating TextDataset\n"]},{"output_type":"stream","name":"stdout","text":["Create TextDataset backing LRO: projects/273872083706/locations/us-central1/datasets/8567278055398047744/operations/8739725252940005376\n"]},{"output_type":"stream","name":"stderr","text":["INFO:google.cloud.aiplatform.datasets.dataset:Create TextDataset backing LRO: projects/273872083706/locations/us-central1/datasets/8567278055398047744/operations/8739725252940005376\n"]},{"output_type":"stream","name":"stdout","text":["TextDataset created. Resource name: projects/273872083706/locations/us-central1/datasets/8567278055398047744\n"]},{"output_type":"stream","name":"stderr","text":["INFO:google.cloud.aiplatform.datasets.dataset:TextDataset created. Resource name: projects/273872083706/locations/us-central1/datasets/8567278055398047744\n"]},{"output_type":"stream","name":"stdout","text":["To use this TextDataset in another session:\n"]},{"output_type":"stream","name":"stderr","text":["INFO:google.cloud.aiplatform.datasets.dataset:To use this TextDataset in another session:\n"]},{"output_type":"stream","name":"stdout","text":["ds = aiplatform.TextDataset('projects/273872083706/locations/us-central1/datasets/8567278055398047744')\n"]},{"output_type":"stream","name":"stderr","text":["INFO:google.cloud.aiplatform.datasets.dataset:ds = aiplatform.TextDataset('projects/273872083706/locations/us-central1/datasets/8567278055398047744')\n"]},{"output_type":"stream","name":"stdout","text":["Importing TextDataset data: projects/273872083706/locations/us-central1/datasets/8567278055398047744\n"]},{"output_type":"stream","name":"stderr","text":["INFO:google.cloud.aiplatform.datasets.dataset:Importing TextDataset data: projects/273872083706/locations/us-central1/datasets/8567278055398047744\n"]},{"output_type":"stream","name":"stdout","text":["Import TextDataset data backing LRO: projects/273872083706/locations/us-central1/datasets/8567278055398047744/operations/3797024661900886016\n"]},{"output_type":"stream","name":"stderr","text":["INFO:google.cloud.aiplatform.datasets.dataset:Import TextDataset data backing LRO: projects/273872083706/locations/us-central1/datasets/8567278055398047744/operations/3797024661900886016\n"]},{"output_type":"stream","name":"stdout","text":["TextDataset data imported. Resource name: projects/273872083706/locations/us-central1/datasets/8567278055398047744\n"]},{"output_type":"stream","name":"stderr","text":["INFO:google.cloud.aiplatform.datasets.dataset:TextDataset data imported. Resource name: projects/273872083706/locations/us-central1/datasets/8567278055398047744\n"]},{"output_type":"stream","name":"stdout","text":["projects/273872083706/locations/us-central1/datasets/8567278055398047744\n"]}],"source":["from datetime import datetime\n","\n","TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n","\n","import google.cloud.aiplatform as aip\n","from google.cloud.aiplatform.datasets.text_dataset import TextDataset\n","dataset : TextDataset = aip.TextDataset.create (\n","    display_name=KAGGLE_DATASET_NAME + \"_\" + TIMESTAMP,\n","    gcs_source= map( lambda x: f\"gs://{BUCKET_NAME}/{x}\", (file_map.values()) ),\n","    import_schema_uri=aip.schema.dataset.ioformat.text.extraction\n",")\n","\n","print(dataset.resource_name)"]}],"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/aburdenko/gcp-jupyter-notebooks/blob/main/ml-apis/Docai_NLP_BQ.ipynb","timestamp":1666365014735},{"file_id":"1_A5jh-1Te8ouC5TYmGb_lQ5T5cU8BG5k","timestamp":1663023158943},{"file_id":"https://github.com/aburdenko/nlp_demo/blob/main/notebooks/nlp_demo.ipynb","timestamp":1659455970725}]},"interpreter":{"hash":"42274a0ef9a3a8c8323d8c1a12a6eaebfc4b2f1c75138184df8e573c9d705262"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"vscode":{"interpreter":{"hash":"31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"}}},"nbformat":4,"nbformat_minor":0}